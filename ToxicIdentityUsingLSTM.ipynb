{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preface","metadata":{}},{"cell_type":"markdown","source":"This kernel is a PyTorch version of the [Simple LSTM kernel](https://www.kaggle.com/thousandvoices/simple-lstm, https://www.kaggle.com/code/bminixhofer/simple-lstm-pytorch-version). All credit for architecture and preprocessing goes to @thousandvoices, BENJAMIN MINIXHOFER .\nThere is a lot of discussion whether Keras, PyTorch, Tensorflow or the CUDA C API is best. But specifically between the PyTorch and Keras version of the simple LSTM architecture, there are 2 clear advantages of PyTorch:\n- Speed. The PyTorch version runs about 20 minutes faster.\n- Determinism. The PyTorch version is fully deterministic. Especially when it gets harder to improve your score later in the competition, determinism is very important.\n\nI was surprised to see that PyTorch is that much faster, so I'm not completely sure the steps taken are exactly the same. If you see any difference, we can discuss it in the comments :)\n\nThe most likely reason the score of this kernel is higher than the @thousandvoices version is that the optimizer is not reinitialized after every epoch and thus the parameter-specific learning rates of Adam are not discarded after every epoch. That is the only difference between the kernels that is intended.","metadata":{}},{"cell_type":"markdown","source":"# Imports & Utility functions","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\nfrom sklearn.model_selection import train_test_split","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-12-06T23:44:11.914065Z","iopub.execute_input":"2022-12-06T23:44:11.914348Z","iopub.status.idle":"2022-12-06T23:44:14.710226Z","shell.execute_reply.started":"2022-12-06T23:44:11.914298Z","shell.execute_reply":"2022-12-06T23:44:14.709221Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","source":"# disable progress bars when submitting\ndef is_interactive():\n   return 'SHLVL' not in os.environ\n\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop","metadata":{"execution":{"iopub.status.busy":"2022-12-06T01:40:09.798122Z","iopub.execute_input":"2022-12-06T01:40:09.798406Z","iopub.status.idle":"2022-12-06T01:40:09.803500Z","shell.execute_reply.started":"2022-12-06T01:40:09.798354Z","shell.execute_reply":"2022-12-06T01:40:09.802784Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T01:40:13.447400Z","iopub.execute_input":"2022-12-06T01:40:13.447937Z","iopub.status.idle":"2022-12-06T01:40:13.457339Z","shell.execute_reply.started":"2022-12-06T01:40:13.447678Z","shell.execute_reply":"2022-12-06T01:40:13.456618Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"CRAWL_EMBEDDING_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nGLOVE_EMBEDDING_PATH = '../input/glove840b300dtxt/glove.840B.300d.txt'\nNUM_MODELS = 2\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nMAX_LEN = 220","metadata":{"execution":{"iopub.status.busy":"2022-12-06T01:40:17.055817Z","iopub.execute_input":"2022-12-06T01:40:17.056127Z","iopub.status.idle":"2022-12-06T01:40:17.060395Z","shell.execute_reply.started":"2022-12-06T01:40:17.056069Z","shell.execute_reply":"2022-12-06T01:40:17.059452Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-12-06T01:40:20.759320Z","iopub.execute_input":"2022-12-06T01:40:20.759603Z","iopub.status.idle":"2022-12-06T01:40:20.765639Z","shell.execute_reply.started":"2022-12-06T01:40:20.759548Z","shell.execute_reply":"2022-12-06T01:40:20.764626Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef train_model(model, train, test, loss_fn, output_dim, lr=0.001,\n                batch_size=512, n_epochs=4,\n                enable_checkpoint_ensemble=True):\n    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n    \n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n    all_test_preds = []\n    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n    \n    for epoch in range(n_epochs):\n        start_time = time.time()\n        \n        scheduler.step()\n        \n        model.train()\n        avg_loss = 0.\n        \n        for data in tqdm(train_loader, disable=False):\n            x_batch = data[:-1]\n            y_batch = data[-1]\n\n            y_pred = model(*x_batch)            \n            loss = loss_fn(y_pred, y_batch)\n\n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n            avg_loss += loss.item() / len(train_loader)\n            \n        model.eval()\n        test_preds = np.zeros((len(test), output_dim))\n    \n        for i, x_batch in enumerate(test_loader):\n            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy())\n\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n\n        all_test_preds.append(test_preds)\n        elapsed_time = time.time() - start_time\n        print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n              epoch + 1, n_epochs, avg_loss, elapsed_time))\n\n    if enable_checkpoint_ensemble:\n        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n    else:\n        test_preds = all_test_preds[-1]\n        \n    return test_preds","metadata":{"execution":{"iopub.status.busy":"2022-12-06T01:40:23.730687Z","iopub.execute_input":"2022-12-06T01:40:23.731023Z","iopub.status.idle":"2022-12-06T01:40:23.740472Z","shell.execute_reply.started":"2022-12-06T01:40:23.730973Z","shell.execute_reply":"2022-12-06T01:40:23.739749Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n    \nclass NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n    \n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n        \n    def forward(self, x):\n        h_embedding = self.embedding(x)\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool), 1)\n        h_conc_linear1  = F.relu(self.linear1(h_conc))\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2022-12-06T01:40:41.197479Z","iopub.execute_input":"2022-12-06T01:40:41.197792Z","iopub.status.idle":"2022-12-06T01:40:41.207209Z","shell.execute_reply.started":"2022-12-06T01:40:41.197718Z","shell.execute_reply":"2022-12-06T01:40:41.206073Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def preprocess(data):\n    '''\n    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n    '''\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-12-06T23:46:28.472319Z","iopub.execute_input":"2022-12-06T23:46:28.472617Z","iopub.status.idle":"2022-12-06T23:46:28.477900Z","shell.execute_reply.started":"2022-12-06T23:46:28.472562Z","shell.execute_reply":"2022-12-06T23:46:28.476891Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"train_complete = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n##test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\ntrain, test = train_test_split(train_complete, test_size=0.2, random_state=25)\nprint(train['comment_text'].head())\n\nx_train = preprocess(train['comment_text'])\nprint(x_train.describe())\nprint(x_train.head())\ny_train = np.where(train['target'] >= 0.5, 1, 0)\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\nx_test = preprocess(test['comment_text'])\ny_test_observed = np.where(test['target'] >= 0.5, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T23:46:38.702326Z","iopub.execute_input":"2022-12-06T23:46:38.702650Z","iopub.status.idle":"2022-12-06T23:47:20.331263Z","shell.execute_reply.started":"2022-12-06T23:46:38.702591Z","shell.execute_reply":"2022-12-06T23:47:20.330412Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"1100418    Your brain is hyperbole if you don't think peo...\n14385      Look ,  why does the state give credits for a ...\n302714     Gail Collins  for The New York Times.  Yet one...\n526404     What exactly is ADN's agenda? Reporting on a n...\n54960      Today's testimony suggests that the reason Hag...\nName: comment_text, dtype: object\ncount        1443899\nunique       1425166\ntop       Well said \nfreq             232\nName: comment_text, dtype: object\n1100418    Your brain is hyperbole if you don t think peo...\n14385      Look    why does the state give credits for a ...\n302714     Gail Collins  for The New York Times   Yet one...\n526404     What exactly is ADN s agenda  Reporting on a n...\n54960      Today s testimony suggests that the reason Hag...\nName: comment_text, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"targetxy = train['target']\nprint(targetxy.describe())\nprint(np.count_nonzero(targetxy < 0.5))\nprint(np.count_nonzero(targetxy >= 0.5))\nprint(\"######## test ######\")\ntarget_test = test['target']\nprint(target_test.describe())\nprint(np.count_nonzero(target_test < 0.5))\nprint(np.count_nonzero(target_test >= 0.5))","metadata":{"execution":{"iopub.status.busy":"2022-12-07T00:16:28.969367Z","iopub.execute_input":"2022-12-07T00:16:28.969654Z","iopub.status.idle":"2022-12-07T00:16:29.051636Z","shell.execute_reply.started":"2022-12-07T00:16:28.969601Z","shell.execute_reply":"2022-12-07T00:16:29.050765Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"count    1.443899e+06\nmean     1.031376e-01\nstd      1.971016e-01\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      1.666667e-01\nmax      1.000000e+00\nName: target, dtype: float64\n1328515\n115384\n######## test ######\ncount    360975.000000\nmean          0.102536\nstd           0.196972\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           0.166667\nmax           1.000000\nName: target, dtype: float64\n332025\n28950\n","output_type":"stream"}]},{"cell_type":"code","source":"max_features = None","metadata":{"execution":{"iopub.status.busy":"2022-12-06T01:42:45.196347Z","iopub.execute_input":"2022-12-06T01:42:45.196648Z","iopub.status.idle":"2022-12-06T01:42:45.202100Z","shell.execute_reply.started":"2022-12-06T01:42:45.196592Z","shell.execute_reply":"2022-12-06T01:42:45.201136Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"tokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-06T01:42:49.755068Z","iopub.execute_input":"2022-12-06T01:42:49.755357Z","iopub.status.idle":"2022-12-06T01:45:56.184927Z","shell.execute_reply.started":"2022-12-06T01:42:49.755303Z","shell.execute_reply":"2022-12-06T01:45:56.184093Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print(x_test[1:5])","metadata":{"execution":{"iopub.status.busy":"2022-12-06T01:47:46.462805Z","iopub.execute_input":"2022-12-06T01:47:46.463098Z","iopub.status.idle":"2022-12-06T01:47:46.471843Z","shell.execute_reply.started":"2022-12-06T01:47:46.463045Z","shell.execute_reply":"2022-12-06T01:47:46.470714Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[[    0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     8\n    512    17     1   630    18  6741    25  6182     3     1  3385   228\n  10650    42    19     1  3327   534    12    75    28     9  1870     2\n     16   451    60   154]\n [    0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0   333   172     1  6090   209     4     1   366\n      6  3082 80297     1   347 46291    43    15   820     1 48260     4\n     31    20    19  5081     3    41  1705    29    69    20  1629     1\n   8248    37    33  7114  2068  4365    10    44    53     2    80    31\n      6  2661    45    62   263     3    78     9    46    67  6740     9\n    145    96    16     8   183  4406    22     9    28  5641   111    15\n     15    94    62   263  2661   308   518     9    18   371    36     8\n     10    41   162   194     1  8664     2     9   808    10   202  4803\n      8     1   784     6  4558     7     1 48651     4     1   711     4\n    196     3   468   579]\n [    0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0    10   114    15   173    10   298    21   296     1\n   2125     6   951     2  8561    43     7  1142    45     5   347 12588\n  28495    35   145    16   409     7  1142   130  5620     1    66   849\n     90   151 22191  1349  2125    21   347 12588  7937   294    67   160\n     48  9131  1957     3    82  2283    37     8   447    13  5165  1210\n     90   151     2    21  2125    13  7971     7     1  5744    27    13\n    115   951     2    42   165     2 17879     5  2342   754    35 17118\n      5  1957     8    86    67  1981    15  1674   888    25   499  1274\n    457    22  4458     4  3333    18    50  2323   115   151 22191  1349\n   2125     4     1  5744]\n [    0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0   105   283]]\n","output_type":"stream"}]},{"cell_type":"code","source":"max_features = max_features or len(tokenizer.word_index) + 1\nmax_features","metadata":{"execution":{"iopub.status.busy":"2022-12-06T02:02:42.909309Z","iopub.execute_input":"2022-12-06T02:02:42.909600Z","iopub.status.idle":"2022-12-06T02:02:42.916484Z","shell.execute_reply.started":"2022-12-06T02:02:42.909548Z","shell.execute_reply":"2022-12-06T02:02:42.915736Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"317954"},"metadata":{}}]},{"cell_type":"code","source":"crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(crawl_matrix))\nprint('n unknown words (unknown_words_crawl): ', len(unknown_words_crawl))","metadata":{"execution":{"iopub.status.busy":"2022-12-06T02:02:59.365034Z","iopub.execute_input":"2022-12-06T02:02:59.365317Z","iopub.status.idle":"2022-12-06T02:05:25.825404Z","shell.execute_reply.started":"2022-12-06T02:02:59.365266Z","shell.execute_reply":"2022-12-06T02:05:25.824634Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"n unknown words (crawl):  317954\nn unknown words (unknown_words_crawl):  166801\n","output_type":"stream"}]},{"cell_type":"code","source":"glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))","metadata":{"execution":{"iopub.status.busy":"2022-12-05T05:47:25.564221Z","iopub.execute_input":"2022-12-05T05:47:25.564685Z","iopub.status.idle":"2022-12-05T05:50:12.207999Z","shell.execute_reply.started":"2022-12-05T05:47:25.564631Z","shell.execute_reply":"2022-12-05T05:50:12.207186Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"n unknown words (glove):  163577\n","output_type":"stream"}]},{"cell_type":"code","source":"embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix.shape\n\ndel crawl_matrix\ndel glove_matrix\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-12-05T05:50:12.209328Z","iopub.execute_input":"2022-12-05T05:50:12.209621Z","iopub.status.idle":"2022-12-05T05:50:12.857145Z","shell.execute_reply.started":"2022-12-05T05:50:12.209573Z","shell.execute_reply":"2022-12-05T05:50:12.856361Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"x_train_torch = torch.tensor(x_train, dtype=torch.long).cuda()\nx_test_torch = torch.tensor(x_test, dtype=torch.long).cuda()\ny_train_torch = torch.tensor(np.hstack([y_train[:, np.newaxis], y_aux_train]), dtype=torch.float32).cuda()","metadata":{"execution":{"iopub.status.busy":"2022-12-05T05:58:42.299357Z","iopub.execute_input":"2022-12-05T05:58:42.299645Z","iopub.status.idle":"2022-12-05T05:58:50.289766Z","shell.execute_reply.started":"2022-12-05T05:58:42.299593Z","shell.execute_reply":"2022-12-05T05:58:50.288878Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"\ntrain_dataset = data.TensorDataset(x_train_torch, y_train_torch)\ntest_dataset = data.TensorDataset(x_test_torch)\n\nall_test_preds = []\n\nfor model_idx in range(NUM_MODELS):\n    print('Model ', model_idx)\n    seed_everything(1234 + model_idx)\n    \n    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n    model.cuda()\n    \n    test_preds = train_model(model, train_dataset, test_dataset, output_dim=y_train_torch.shape[-1], \n                             loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n    all_test_preds.append(test_preds)\n    print()","metadata":{"execution":{"iopub.status.busy":"2022-12-05T05:59:18.617697Z","iopub.execute_input":"2022-12-05T05:59:18.617996Z","iopub.status.idle":"2022-12-05T07:06:37.888020Z","shell.execute_reply.started":"2022-12-05T05:59:18.617939Z","shell.execute_reply":"2022-12-05T07:06:37.887179Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Model  0\nEpoch 1/4 \t loss=0.1107 \t time=503.30s\nEpoch 2/4 \t loss=0.1040 \t time=501.01s\nEpoch 3/4 \t loss=0.1024 \t time=505.55s\nEpoch 4/4 \t loss=0.1014 \t time=502.66s\n\nModel  1\nEpoch 1/4 \t loss=0.1107 \t time=509.16s\nEpoch 2/4 \t loss=0.1039 \t time=506.00s\nEpoch 3/4 \t loss=0.1024 \t time=504.55s\nEpoch 4/4 \t loss=0.1014 \t time=499.91s\n\n","output_type":"stream"}]},{"cell_type":"code","source":"test_predictions = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n})\n\n\n# submission.to_csv('submission.csv', index=False)\n\n\n# from sklearn import metrics\n\n# def print_report(pipe, x_test, y_test):\n#     y_pred = pipe.predict(x_test)\n#     report = metrics.classification_report(y_test, y_pred)\n#     print(report)\n#     print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))\n\n# print_report(pipe, x_test, y_test)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-05T07:06:38.153612Z","iopub.status.idle":"2022-12-05T07:06:38.154572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(test_predictions.shape)\nprint(test.shape)\n\nprint(y_test_observed.shape)\n\ny_prediction = np.where(test_predictions[\"prediction\"] >= 0.5, 1, 0) \n\nprint(y_prediction.shape)\n\nfrom sklearn import metrics\n\ndef print_report(y_test, y_pred):\n\n    report = metrics.classification_report(y_test, y_pred)\n    print(report)\n    print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))\n\nprint_report(y_test_observed, y_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-12-05T07:31:06.707709Z","iopub.execute_input":"2022-12-05T07:31:06.708067Z","iopub.status.idle":"2022-12-05T07:31:07.139848Z","shell.execute_reply.started":"2022-12-05T07:31:06.708020Z","shell.execute_reply":"2022-12-05T07:31:07.139116Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"(360975, 2)\n(360975, 45)\n(360975,)\n(360975,)\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.98    332025\n           1       0.75      0.64      0.70     28950\n\n   micro avg       0.95      0.95      0.95    360975\n   macro avg       0.86      0.81      0.84    360975\nweighted avg       0.95      0.95      0.95    360975\n\naccuracy: 0.955\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Note that the solution is not validated in this kernel. So for tuning anything, you should build a validation framework using e. g. KFold CV. If you just check what works best by submitting, you are very likely to overfit to the public LB.","metadata":{"trusted":true}},{"cell_type":"markdown","source":"# Ways to improve this kernel","metadata":{}},{"cell_type":"markdown","source":"This kernel is just a simple baseline kernel, so there are many ways to improve it. Some ideas to get you started:\n- Add a contraction mapping. E. g. mapping \"is'nt\" to \"is not\" can help the network because \"not\" is explicitly mentioned. They were very popular in the recent quora competition, see for example [this kernel](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing).\n- Try to reduce the number of words that are not found in the embeddings. At the moment, around 170k words are not found. We can take some steps to decrease this amount, for example trying to find a vector for a processed (capitalized, stemmed, ...) version of the word when the vector for the regular word can not be found. See the [3rd place solution](https://www.kaggle.com/wowfattie/3rd-place) of the quora competition for an excellent implementation of this.\n- Try cyclic learning rate (CLR). I have found CLR to almost always improve my network recently compared to the default parameters for Adam. In this case, we are already using a learning rate scheduler, so this might not be the case. But it is still worth to try it out. See for example my [my other PyTorch kernel](https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch) for an implementation of CLR in PyTorch.\n- Use sequence bucketing to train faster and fit more networks into the two hours. The winning team of the quora competition successfully used sequence bucketing to drastically reduce the time it took to train RNNs. An excerpt from their [solution summary](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80568#latest-487092):\n\n> We aimed at combining as many models as possible. To do this, we needed to improve runtime and the most important thing to achieve this was the following. We do not pad sequences to the same length based on the whole data, but just on a batch level. That means we conduct padding and truncation on the data generator level for each batch separately, so that length of the sentences in a batch can vary in size. Additionally, we further improved this by not truncating based on the length of the longest sequence in the batch, but based on the 95% percentile of lengths within the sequence. This improved runtime heavily and kept accuracy quite robust on single model level, and improved it by being able to average more models.\n\n- Try a (weighted) average of embeddings instead of concatenating them. A 600d vector for each word is a lot, it might work better to average them instead. See [this paper](https://www.aclweb.org/anthology/N18-2031) for why this even works.\n- Limit the maximum number of words used to train the NN. At the moment, there is no limit set to the maximum number of words in the tokenizer, so we use every word that occurs in the training data, even if it is only mentioned once. This could lead to overfitting so it might be better to limit the maximum number of words to e. g. 100k.\n\nThanks for reading. Good luck and have fun in this competition!","metadata":{}}]}